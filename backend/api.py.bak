from app.backend.retriever import load_graph, retrieve
from app.backend.composer import build_messages
from app.backend.llm_client import chat
from app.backend.brain import answer_from_graph


load_dotenv()
load_graph()

app = FastAPI(title="Graph LLM–KG API")

class AskBody(BaseModel):
    question: str
    topk_paths: int = int(os.getenv("TOPK_PATHS", "5"))
    max_hops: int = int(os.getenv("MAX_HOPS", "3"))
    neighbor_expand: int = int(os.getenv("NEIGHBOR_EXPAND", "2"))

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/ask")
async def ask(body: AskBody, dry_run: bool = Query(default=False)):
    try:
        ctx = retrieve(body.question, body.topk_paths, body.max_hops, body.neighbor_expand)
        if dry_run:
            return {"answer": None, "ctx": ctx, "note": "dry_run (LLM skipped)"}

        # 1) Try graph-only rule for instant answers
        rule_ans = answer_from_graph(body.question, ctx)
        if rule_ans:
            return {"answer": rule_ans, "ctx": ctx, "source": "graph-rule"}

        # 2) Fall back to LLM for phrasing
        msgs = build_messages(body.question, ctx)
        t0 = time.time()
        ans = await chat(msgs)
        t1 = time.time()
        return {"answer": ans, "ctx": ctx, "latency_s": round(t1 - t0, 3), "source": "llm"}
    except Exception as e:
        return {"error": str(e)}
# === PATCH START (reranker wiring) ===
# Add near other imports:
try:
    from src.rerank import RapidBM25Reranker, RerankConfig
except Exception:
    RapidBM25Reranker = None
    RerankConfig = None

# In your Pydantic models (add fields if you use a model for request):
# Example:
# class AskRequest(BaseModel):
#     question: str
#     topk_paths: int = 5
#     max_hops: int = 3
#     neighbor_expand: int = 2
#     use_llm: bool = True
#     answer_style: str = "concise"
#     explain: bool = True
#     use_rerank: bool = True
#     rerank_mode: str = "hybrid"
#     rerank_topk: Optional[int] = None

# Inside your /ask handler, AFTER you’ve produced `result` with fields:
#   result["paths"] -> List[List[step]]
#   result["node_notes"] -> List[str]
# and BEFORE returning to client, insert:

def _apply_rerank_if_enabled(question, result, use_rerank: bool, rerank_mode: str, rerank_topk: int | None):
    if not use_rerank or not RapidBM25Reranker:
        return result
    paths = result.get("paths") or []
    notes = result.get("node_notes") or []
    if not paths:
        return result
    cfg = RerankConfig(mode=rerank_mode or "hybrid")
    rr = RapidBM25Reranker(cfg)
    order, scores = rr.rerank(question, notes, paths)
    new_paths = [paths[i] for i in order]
    new_scores = [scores[i] for i in order]
    if rerank_topk:
        new_paths = new_paths[:rerank_topk]
        new_scores = new_scores[:rerank_topk]
    result["paths"] = new_paths
    result["path_scores"] = new_scores
    result["meta"] = {**(result.get("meta") or {}), "reranked": True, "rerank_mode": cfg.mode}
    return result

# In the actual handler:
# req = AskRequest(**payload)
# ...
# result = do_graph_search(...)
# ...
result = _apply_rerank_if_enabled(
    question=req.question,
    result=result,
    use_rerank=getattr(req, "use_rerank", True),
    rerank_mode=getattr(req, "rerank_mode", "hybrid"),
    rerank_topk=getattr(req, "rerank_topk", None) or getattr(req, "topk_paths", None)
)
# === PATCH END ===
